{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965afde7-91e0-4199-a998-5a55a727842d",
   "metadata": {},
   "source": [
    "# Key Areas in NLP\n",
    "**Text Preprocessing** â€“ cleaning raw text (removing stop words, stemming, lemmatization, tokenization).\n",
    "\n",
    "**Text Representation** â€“ converting words into numbers (Bag of Words, TF-IDF, Word2Vec, embeddings).\n",
    "\n",
    "**Text Classification** â€“ spam detection, sentiment analysis, topic labeling.\n",
    "\n",
    "**Named Entity Recognition (NER)** â€“ identifying names, locations, organizations in text.\n",
    "\n",
    "**Machine Translation** â€“ Google Translate, DeepL.\n",
    "\n",
    "**Text Summarization** â€“ generating summaries from large documents.\n",
    "\n",
    "**Question Answering / Chatbots** â€“ like me ðŸ™‚\n",
    "\n",
    "**Speech Processing**â€“ speech-to-text, voice assistants (Alexa, Siri)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00230361-c0e5-4498-b31a-6117ba8d4e5f",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "- Raw text contains noise (punctuations, stopwords, different cases, numbers).\n",
    "- Preprocessing improves accuracy in NLP tasks (classification, sentiment analysis, etc).\n",
    "- \n",
    "**Steps in Preprocessing**\n",
    "\n",
    "  1. Tokenization\n",
    "\n",
    "  2. Lowercasing \n",
    "\n",
    "  3. Stop words\n",
    " \n",
    "  4. Removing punctuations/numbers\n",
    " \n",
    "  5. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c08a71-97fc-4c0d-b338-9a593e350f42",
   "metadata": {},
   "source": [
    "# Overall Benefits of Preprocessing\n",
    "**Removes noise** (unnecessary words, punctuation).\n",
    "\n",
    "**Reduces complexity** (smaller vocabulary â†’ faster & more efficient models).\n",
    "\n",
    "**Improves accuracy** (focuses model on meaningful patterns).\n",
    "\n",
    "**Standardizes text** (so different forms of the same word are treated equally)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a57de-2dec-4910-8765-47bf5cd710c8",
   "metadata": {},
   "source": [
    "\n",
    "**Tokanization using NLTK**: (natural language tool kit)\n",
    "splitting text into smaller units\n",
    "\n",
    "**NLTK** : simple, rule based tokenizer.\n",
    "\n",
    "**spaCy** : More advanced - handles punctuation, named entities, and linguistic features better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "068be4f8-5809-4873-948c-ce711c849188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Gundluru\n",
      "[nltk_data]     Madhura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization:  ['Natural Language Processing is amazing.', 'It helps computers understand human language!']\n",
      "Word Tokenization:  ['Natural', 'Language', 'Processing', 'is', 'amazing', '.', 'It', 'helps', 'computers', 'understand', 'human', 'language', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text =\"Natural Language Processing is amazing. It helps computers understand human language!\"\n",
    "\n",
    "#sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokenization: \",sentences)\n",
    "\n",
    "#Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"Word Tokenization: \",words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea85f3c8-5647-4966-ad43-2d8812489f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization:  ['Natural Language Processing is amazing.', 'It helps computers understand human language!']\n",
      "Word Tokenization:  ['Natural', 'Language', 'Processing', 'is', 'amazing', '.', 'It', 'helps', 'computers', 'understand', 'human', 'language', '!']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#load english tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text =\"Natural Language Processing is amazing. It helps computers understand human language!\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#sentence Tokenization\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "print(\"Sentence Tokenization: \",sentences)\n",
    "\n",
    "#Word Tokenization\n",
    "words = [token.text for token in doc]\n",
    "print(\"Word Tokenization: \",words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "badbeba5-2f2e-44f7-ae29-631c1beac5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: {'amazing', 'language', 'helps', 'understand', 'processing', '.', 'is', 'natural', 'computers'}\n",
      "Duplicate words with counts (mode): {'natural': 2, 'language': 3, '.': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Gundluru\n",
      "[nltk_data]     Madhura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#using nltk find the unique words present in a sentence and duplicate words with their mode ( no. of occurences)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Natural Language Processing is amazing. Natural language helps computers understand language.\"\n",
    "\n",
    "# Tokenization\n",
    "words = word_tokenize(text.lower())\n",
    "\n",
    "# Unique words\n",
    "unique_words = set(words)\n",
    "print(\"Unique words:\", unique_words)\n",
    "\n",
    "# Duplicate words with their frequency (mode)\n",
    "word_counts = Counter(words)\n",
    "duplicates = {word: count for word, count in word_counts.items() if count > 1}\n",
    "print(\"Duplicate words with counts (mode):\", duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb5ca864-f6bb-4198-a1a0-f0390e9ba899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Artificial Intelligence is transforming the world.', 'AI helps in healthcare, finance, education, and more.', 'The future with AI looks exciting!']\n",
      "Words: ['artificial', 'intelligence', 'is', 'transforming', 'the', 'world', 'ai', 'helps', 'in', 'healthcare', 'finance', 'education', 'and', 'more', 'the', 'future', 'with', 'ai', 'looks', 'exciting']\n",
      "\n",
      "Total Sentences: 3\n",
      "Total Words: 20\n",
      "\n",
      "Unique Words and Frequency:\n",
      "artificial: 1\n",
      "intelligence: 1\n",
      "is: 1\n",
      "transforming: 1\n",
      "the: 2\n",
      "world: 1\n",
      "ai: 2\n",
      "helps: 1\n",
      "in: 1\n",
      "healthcare: 1\n",
      "finance: 1\n",
      "education: 1\n",
      "and: 1\n",
      "more: 1\n",
      "future: 1\n",
      "with: 1\n",
      "looks: 1\n",
      "exciting: 1\n"
     ]
    }
   ],
   "source": [
    "# another way of counting dupicates\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample paragraph\n",
    "text = \"\"\"Artificial Intelligence is transforming the world. \n",
    "AI helps in healthcare, finance, education, and more. \n",
    "The future with AI looks exciting!\"\"\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = [sent.text.strip() for sent in doc.sents]\n",
    "print(\"Sentences:\", sentences)\n",
    "\n",
    "# Word Tokenization (ignoring punctuation & spaces)\n",
    "words = [token.text.lower() for token in doc if not token.is_punct and not token.is_space]\n",
    "print(\"Words:\", words)\n",
    "\n",
    "# Count sentences & words\n",
    "print(\"\\nTotal Sentences:\", len(sentences))\n",
    "print(\"Total Words:\", len(words))\n",
    "\n",
    "# Unique words with frequency\n",
    "word_freq = Counter(words)\n",
    "print(\"\\nUnique Words and Frequency:\")\n",
    "for word, freq in word_freq.items():\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a3be21-681c-4ff6-b177-52bbbde5e499",
   "metadata": {},
   "source": [
    "# Stop words/Cut words\n",
    "\"Stop words\" are common words in a language (like the, is, in, at, of, and) that usually donâ€™t carry important meaning in text analysis.\n",
    "\n",
    "In Natural Language Processing (NLP), stop words are often removed before processing text because:\n",
    "\n",
    "They occur very frequently.\n",
    "\n",
    "They donâ€™t add much value for tasks like text classification, search engines, or keyword extraction.\n",
    "\n",
    "**Examples of English Stop Words**\n",
    "\n",
    "**Articles**: a, an, the\n",
    "\n",
    "**Pronouns**: I, you, he, she, it, we, they\n",
    "\n",
    "**Prepositions**: on, in, at, under, over\n",
    "\n",
    "**Conjunctions**: and, or, but, because\n",
    "\n",
    "**Auxiliary Verbs**: is, was, were, has, have, do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db52ba87-ece5-4e6e-9b3d-1e0d7b7951de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  11\n",
      "stop words count:  6\n",
      "Original:  ['This', 'is', 'an', 'example', 'sentence', 'showing', 'off', 'the', 'stop', 'words', 'filtration']\n",
      "Without stop words:  ['example', 'sentence', 'showing', 'stop', 'words', 'filtration']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Gundluru\n",
      "[nltk_data]     Madhura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "words = word_tokenize(\"This is an example sentence showing off the stop words filtration\")\n",
    "filtered_words = [w for w in words if w.lower() not in stop_words]\n",
    "\n",
    "# count the how many stop words and words are there\n",
    "stop_words_count = sum(1 for w in words if w.lower() not in stop_words)\n",
    "print(\"Total words: \",len(words))\n",
    "print(\"stop words count: \",stop_words_count)\n",
    "\n",
    "print(\"Original: \",words)\n",
    "print(\"Without stop words: \",filtered_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadbc431-28b2-4eb3-aa20-5d6e65e59acb",
   "metadata": {},
   "source": [
    "# Removing punctuations/numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0218f37-33f9-4ebc-88c0-8798372478a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned:  Hello NLP is great \n",
      "After stopwords:  ['Hello', 'NLP', 'great']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Gundluru\n",
      "[nltk_data]     Madhura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#regular expression to check the text with specific patterns\n",
    "# eg : extract only alphabets by eleminating numbers and symbols using re.sub()\n",
    "#removing punctuations/numbers\n",
    "import re\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "text = \"Hello!! NLP123 is great :)\"\n",
    "cleaned = re.sub(r'[^a-zA-Z\\s]','',text) # expression to remove punctuations/numbers\n",
    "print(\"cleaned: \",cleaned)\n",
    "words = word_tokenize(cleaned)\n",
    "stopwords = [w for w in words if w not in stop_words]#comphrehence expressions(one line expression)\n",
    "print(\"After stopwords: \",stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43fabef-a587-4853-92dc-2af02f8d753e",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "**Stemming**: Cuts words to their base/root form(fast but rough).\n",
    "\n",
    "    - \"Studies\" -> \"studi\",\"playing\" -> \"play\".\n",
    "    \n",
    "**Lemmatization** : Reduces words to dictionary form using grammar(accurate).\n",
    "\n",
    "    - \"studies\" -> \"study\",\"playing\" -> \"play\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c680c8f0-1882-45a2-80ed-de8948080a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Gundluru\n",
      "[nltk_data]     Madhura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Gundluru\n",
      "[nltk_data]     Madhura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: \n",
      "['studi', 'studi', 'better', 'run']\n",
      "Lemmatization: \n",
      "['study', 'studying', 'better', 'running']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"studies\",\"studying\",\"better\",\"running\"]\n",
    "\n",
    "print(\"Stemming: \")\n",
    "print([stemmer.stem(w) for w in words])\n",
    "\n",
    "print(\"Lemmatization: \")\n",
    "print([lemmatizer.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1284899-bd90-49c4-83f6-e7f8c4e45d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Gundluru\n",
      "[nltk_data]     Madhura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Gundluru\n",
      "[nltk_data]     Madhura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Gundluru\n",
      "[nltk_data]     Madhura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Gundluru Madhura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('studies', 'NNS'), ('studying', 'VBG'), ('better', 'RBR'), ('running', 'VBG')]\n",
      "Lemmatized: ['study', 'study', 'well', 'run']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Downloads\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Convert POS tags to WordNet format\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "words = [\"studies\", \"studying\", \"better\", \"running\"]\n",
    "\n",
    "# POS tagging\n",
    "pos_tags = nltk.pos_tag(words, lang=\"eng\")\n",
    "\n",
    "# Lemmatization with POS\n",
    "lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(t)) for w, t in pos_tags]\n",
    "\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "print(\"Lemmatized:\", lemmatized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4966a-ebe2-4e63-b4c5-f4eace58e234",
   "metadata": {},
   "source": [
    "# Text Representation\n",
    "\n",
    "**Why Text Representation?**\n",
    "\n",
    "  - Models requires numbers, not raw text.\n",
    "  - Machines understand numbers, not words.\n",
    "\n",
    "  - Representation helps models learn patterns like similarity, sentiment, topics, etc.\n",
    "\n",
    "Example:\n",
    "\n",
    "Raw text â†’ \"I love NLP\"\n",
    "\n",
    "After BoW â†’ [1,1,1,0,...]\n",
    "\n",
    "After embeddings â†’ [0.21, -0.34, 0.78, ...]\n",
    "  \n",
    "\n",
    "**Bag of Words(BoW)**\n",
    "\n",
    "  - Count the frequency of words.\n",
    "  - Represents text as a bag (collection) of words, ignoring grammar & order.\n",
    "\n",
    "  - Just counts how many times each word appears.\n",
    "  - Ex:\n",
    "    sentences :\n",
    "    \n",
    "        1. \"I like NLP\"\n",
    "    \n",
    "        2. \"I like AI\"\n",
    "\n",
    "Vocabulary = [I,like,NLP,AI]\n",
    "\n",
    "Vectors :\n",
    "\n",
    "      - s1 -> [1,1,1,0]\n",
    "      \n",
    "      - s2 -> [1,1,0,1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aaab65-929f-4a0c-80f6-848a640aea0f",
   "metadata": {},
   "source": [
    "# NLP Mini Project â€” Spam vs Ham Classification\n",
    "\n",
    "**Overview:** Build a simple end-to-end NLP pipeline:\n",
    "1. Load dataset (or use sample data),\n",
    "2. Clean text,\n",
    "3. Convert text to TF-IDF features,\n",
    "4. Train a classifier (Multinomial Naive Bayes),\n",
    "5. Evaluate and predict on new messages.\n",
    "\n",
    "**Goal:** Have a complete, resume-friendly project that can be pushed to GitHub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad512b33-e954-428b-a9e5-9eeb271c8bbf",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab6c9503-d61c-44dc-bbf6-381b0cc7d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fb38de-f418-4ca0-9985-c4564c8b9c8d",
   "metadata": {},
   "source": [
    "# Step 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ba0886e-edb5-483b-b9a4-22fc44fc0fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load SMS Spam Collection dataset\n",
    "df = pd.read_csv(\"smsspamcollection\", sep=\"\\t\", names=[\"label\", \"message\"])\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb8d73-117c-414d-9a3b-5176dd9a2754",
   "metadata": {},
   "source": [
    "# Step 3: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63db8a95-55da-46f2-bca0-ed7cca7dc52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_num\n",
      "0    4825\n",
      "1     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Encode labels: ham -> 0, spam -> 1\n",
    "df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Basic text cleaning (convert to lowercase)\n",
    "df['clean_message'] = df['message'].str.lower()\n",
    "\n",
    "# Check class distribution\n",
    "print(df['label_num'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaebd4a-99c0-4d55-aafc-23712f484b65",
   "metadata": {},
   "source": [
    "# Step 4: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e911e38-6d5d-4d88-96ff-272bfdea6e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4457  Test size: 1115\n",
      "Train class distribution:\n",
      " label_num\n",
      "0    0.865829\n",
      "1    0.134171\n",
      "Name: proportion, dtype: float64\n",
      "Test class distribution:\n",
      " label_num\n",
      "0    0.866368\n",
      "1    0.133632\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = df['clean_message']\n",
    "y = df['label_num']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0], \" Test size:\", X_test.shape[0])\n",
    "print(\"Train class distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"Test class distribution:\\n\", y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d183d27c-5b9f-488a-956e-f9e00740524f",
   "metadata": {},
   "source": [
    "# Step 5: Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52c70eab-9f25-4340-9715-36b473f19d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 7668\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94f4866-04bd-4bf3-a640-f7bde196b13d",
   "metadata": {},
   "source": [
    "# Step 6: Train Model (Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb8e90bb-0219-489e-a28b-3e4f38ab508a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a354df-d86f-4f58-9882-b8e6263519d7",
   "metadata": {},
   "source": [
    "# Step 7: Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4aa8680-b881-4101-95ba-31eb9584d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1c1c9-7009-41e3-bf2e-3a89934d29d2",
   "metadata": {},
   "source": [
    "# Step 8: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49ddad75-ce7b-47b0-89dd-1f3d33e49ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Accuracy: 0.9874439461883409\n",
      "\n",
      "ðŸ“Š Confusion Matrix:\n",
      " [[964   2]\n",
      " [ 12 137]]\n",
      "\n",
      "ðŸ“„ Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       966\n",
      "           1       0.99      0.92      0.95       149\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.99      0.96      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"âœ… Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nðŸ“Š Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nðŸ“„ Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475e3af-41d6-4534-9d6f-c619b79d64f1",
   "metadata": {},
   "source": [
    "# Step 9: Test on New Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b01bda96-029b-4193-a78f-7318c559c870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Congratulations! You won a free ticket to Bahamas. Claim now. -> Spam\n",
      "Message: Hey, are we still meeting today? -> Ham\n",
      "Message: Win a free iPhone now! Click here. -> Spam\n",
      "Message: Can we talk later today? -> Ham\n"
     ]
    }
   ],
   "source": [
    "sample_msgs = [\n",
    "    \"Congratulations! You won a free ticket to Bahamas. Claim now.\",\n",
    "    \"Hey, are we still meeting today?\",\n",
    "    \"Win a free iPhone now! Click here.\",\n",
    "    \"Can we talk later today?\"\n",
    "]\n",
    "\n",
    "sample_vec = vectorizer.transform(sample_msgs)\n",
    "predictions = model.predict(sample_vec)\n",
    "\n",
    "for msg, pred in zip(sample_msgs, predictions):\n",
    "    print(f\"Message: {msg} -> {'Spam' if pred==1 else 'Ham'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca533b4-e95f-45f6-bea1-3e5eb617305e",
   "metadata": {},
   "source": [
    "# Step 10: Save Model & Vectorizer (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e072a7ef-b8dd-4a73-8ab3-7ff648285adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer saved!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, \"spam_classifier_model.pkl\")\n",
    "joblib.dump(vectorizer, \"vectorizer.pkl\")\n",
    "\n",
    "print(\"Model and vectorizer saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba0a72-3675-4fbc-bd34-59d8f7ebfa0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spacyenv)",
   "language": "python",
   "name": "spacyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
